# -*- coding: utf-8 -*-
"""Task 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/192vr8Cllb3waVIhlfO96pdREG83MQVtD

# **Topic modeling on BBC News**

Import necessary libraries
"""

!pip install gensim pyLDAvis wordcloud nltk spacy scikit-learn

import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from gensim import corpora
from nltk.stem import PorterStemmer
from gensim.models import LdaModel
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF


nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

"""Load the dataset

"""

df = pd.read_csv("BBCNews.csv")
print("Dataset loaded successfully!")
print(df.head())
print(df.columns)
texts = df['descr']

"""Preprocessing the text

"""

stop_words = set(stopwords.words('english'))
custom_stopwords = {"said", "news", "report", "will","also", "one", "can"}
lemmatizer = WordNetLemmatizer()

def process(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove punctuation & digits
    tokens = nltk.word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words and word not in custom_stopwords]
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return tokens

# Apply preprocessing
processed_news = [process(doc) for doc in texts]

"""Creating dictionary and corpus"""

dictionary = corpora.Dictionary(processed_news)
corpus = [dictionary.doc2bow(doc) for doc in processed_news]
print("Sample dictionary (first 10 words):")
print(list(dictionary.items())[:10])

print("\nSample corpus (first document):")
print(corpus[0])

"""Train the LDA Model"""

lda_model = LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=5,
    passes=10,
    random_state=42
)

""" Show TOP discovered topics"""

print("TOP Words Per Topic:\n")
for i, topic in lda_model.print_topics():
    print(f"Topic {i}: {topic}\n")

"""Word Clouds per topic

"""

for t in range(lda_model.num_topics):
    plt.figure(figsize=(6, 4))
    plt.imshow(WordCloud(background_color='white').fit_words(dict(lda_model.show_topic(t, 50))))
    plt.axis("off")
    plt.title(f"Topic #{t}")
    plt.show()

"""# ***# BONUS***

NMF Modeling
"""

# Join tokens back to strings for vectorizer
joined_docs = [' '.join(doc) for doc in processed_news]

# TF-IDF matrix
tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')
tfidf = tfidf_vectorizer.fit_transform(joined_docs)

# NMF model
nmf_model = NMF(n_components=5, random_state=42)
nmf_model.fit(tfidf)

# Display top words per topic
def display_topics(model, feature_names, no_top_words=10):
    for topic_idx, topic in enumerate(model.components_):
        print(f"\n Topic {topic_idx}:")
        print(" + ".join([f"{feature_names[i]}" for i in topic.argsort()[:-no_top_words - 1:-1]]))

tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()
display_topics(nmf_model, tfidf_feature_names)

"""WordClouds for NMF Topics"""

for idx, topic in enumerate(nmf_model.components_):
    plt.figure(figsize=(6, 4))
    word_freq = {tfidf_feature_names[i]: topic[i] for i in topic.argsort()[:-50:-1]}
    plt.imshow(WordCloud(background_color='white').fit_words(word_freq))
    plt.axis("off")
    plt.title(f" NMF Topic #{idx}")
    plt.show()

"""pyLDAvis to visualize topic-word distributions

"""

import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

pyLDAvis.enable_notebook()
vis = gensimvis.prepare(lda_model, corpus, dictionary)
vis